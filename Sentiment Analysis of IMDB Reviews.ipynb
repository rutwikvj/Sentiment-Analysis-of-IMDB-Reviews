{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of IMDB Reviews\n",
    "###### By: MBA(BA) - Group 5\n",
    "- The IMDB dataset consists of 50,000 movie reviews and its sentiment labels (positive or negative). We now proceed to build a model that predicts whether a review is positive or negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries and the Dataset\n",
    "- We now import all the required libraries and import the IMDB dataset which is present in the Keras Package itself. It consists of both the reviews and their corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from tensorflow import keras\n",
    "from keras.datasets import imdb\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Activation, Embedding, Convolution1D, MaxPooling1D, Input, Dense, add, \\\n",
    "                         BatchNormalization, Flatten, Reshape, Concatenate\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We now load the dataset into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = imdb.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We check for the shape of the train and test data for our verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data\n",
      "(25000,)\n",
      "(25000,)\n",
      "\n",
      "test data\n",
      "(25000,)\n",
      "(25000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"train data\")\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(\"\\ntest data\")\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We now check for the classes present in both our test and train datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes in Train Set \n",
      "[0 1]\n",
      "Classes in Test Set \n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Classes in Train Set \")\n",
    "print(numpy.unique(y_train))\n",
    "print(\"Classes in Test Set \")\n",
    "print(numpy.unique(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let us now check for a sample from the x_train dataset and the corresponding label in y_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32] \n",
      "\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0],\"\\n\")\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note that the Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers). Words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data.\n",
    "- From the sample we can see the encoded review and the corresponding y_train value which is 1. We are still unsure whether the corresponding 1 indicates whether it's a positive or a negative review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To address this issue, we now proceed to try and retrieve the word index file mapping the words to the indices using get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = keras.datasets.imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We then reverse the word index to obtain a dictionary mapping the indices to word. This will make it easy for us to decode the sequences. We will only display 20 items from the dictionary for tidiness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(34701, 'fawn'),\n",
       " (52006, 'tsukino'),\n",
       " (52007, 'nunnery'),\n",
       " (16816, 'sonja'),\n",
       " (63951, 'vani'),\n",
       " (1408, 'woods'),\n",
       " (16115, 'spiders'),\n",
       " (2345, 'hanging'),\n",
       " (2289, 'woody'),\n",
       " (52008, 'trawling'),\n",
       " (52009, \"hold's\"),\n",
       " (11307, 'comically'),\n",
       " (40830, 'localized'),\n",
       " (30568, 'disobeying'),\n",
       " (52010, \"'royale\"),\n",
       " (40831, \"harpo's\"),\n",
       " (52011, 'canet'),\n",
       " (19313, 'aileen'),\n",
       " (52012, 'acurately'),\n",
       " (52013, \"diplomat's\")]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverted_word_index = dict((i, word) for (word, i) in word_index.items())\n",
    "list(inverted_word_index.items())[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have now obtained a dictionary mapping the indices to the word. The key is the overall frequency of the word which is the corresponding value in the dictionary.\n",
    "- We now sort the key value pairs (order of frequency) in the dictionary for better understanding. Note that we will display only 200 sorted key value pairs for tidiness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'the') (2, 'and') (3, 'a') (4, 'of') (5, 'to') (6, 'is') (7, 'br') (8, 'in') (9, 'it') (10, 'i') (11, 'this') (12, 'that') (13, 'was') (14, 'as') (15, 'for') (16, 'with') (17, 'movie') (18, 'but') (19, 'film') (20, 'on') (21, 'not') (22, 'you') (23, 'are') (24, 'his') (25, 'have') (26, 'he') (27, 'be') (28, 'one') (29, 'all') (30, 'at') (31, 'by') (32, 'an') (33, 'they') (34, 'who') (35, 'so') (36, 'from') (37, 'like') (38, 'her') (39, 'or') (40, 'just') (41, 'about') (42, \"it's\") (43, 'out') (44, 'has') (45, 'if') (46, 'some') (47, 'there') (48, 'what') (49, 'good') (50, 'more') (51, 'when') (52, 'very') (53, 'up') (54, 'no') (55, 'time') (56, 'she') (57, 'even') (58, 'my') (59, 'would') (60, 'which') (61, 'only') (62, 'story') (63, 'really') (64, 'see') (65, 'their') (66, 'had') (67, 'can') (68, 'were') (69, 'me') (70, 'well') (71, 'than') (72, 'we') (73, 'much') (74, 'been') (75, 'bad') (76, 'get') (77, 'will') (78, 'do') (79, 'also') (80, 'into') (81, 'people') (82, 'other') (83, 'first') (84, 'great') (85, 'because') (86, 'how') (87, 'him') (88, 'most') (89, \"don't\") (90, 'made') (91, 'its') (92, 'then') (93, 'way') (94, 'make') (95, 'them') (96, 'too') (97, 'could') (98, 'any') (99, 'movies') (100, 'after') (101, 'think') (102, 'characters') (103, 'watch') (104, 'two') (105, 'films') (106, 'character') (107, 'seen') (108, 'many') (109, 'being') (110, 'life') (111, 'plot') (112, 'never') (113, 'acting') (114, 'little') (115, 'best') (116, 'love') (117, 'over') (118, 'where') (119, 'did') (120, 'show') (121, 'know') (122, 'off') (123, 'ever') (124, 'does') (125, 'better') (126, 'your') (127, 'end') (128, 'still') (129, 'man') (130, 'here') (131, 'these') (132, 'say') (133, 'scene') (134, 'while') (135, 'why') (136, 'scenes') (137, 'go') (138, 'such') (139, 'something') (140, 'through') (141, 'should') (142, 'back') (143, \"i'm\") (144, 'real') (145, 'those') (146, 'watching') (147, 'now') (148, 'though') (149, \"doesn't\") (150, 'years') (151, 'old') (152, 'thing') (153, 'actors') (154, 'work') (155, '10') (156, 'before') (157, 'another') (158, \"didn't\") (159, 'new') (160, 'funny') (161, 'nothing') (162, 'actually') (163, 'makes') (164, 'director') (165, 'look') (166, 'find') (167, 'going') (168, 'few') (169, 'same') (170, 'part') (171, 'again') (172, 'every') (173, 'lot') (174, 'cast') (175, 'us') (176, 'quite') (177, 'down') (178, 'want') (179, 'world') (180, 'things') (181, 'pretty') (182, 'young') (183, 'seems') (184, 'around') (185, 'got') (186, 'horror') (187, 'however') (188, \"can't\") (189, 'fact') (190, 'take') (191, 'big') (192, 'enough') (193, 'long') (194, 'thought') (195, \"that's\") (196, 'both') (197, 'between') (198, 'series') (199, 'give') (200, 'may') "
     ]
    }
   ],
   "source": [
    "for i in sorted (inverted_word_index) :\n",
    "    if i > 200:\n",
    "        break\n",
    "    print ((i, inverted_word_index[i]), end =\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can now see the most frequent words in the review at the top and the least frequent ones at the bottom. \"The\" is the most frequent word used in the reviews according to its index followed by \"and\", \"a\", etc.\n",
    "\n",
    "##### Let us now proceed to decode the sample that we had earlier selected: X_train[0]\n",
    "\n",
    "- We also need to keep in mind that the indices are off by 3 as 0,1, and 2 are specially reserved for padding, start of sequence and unknown. This is done in most of the encoded data where words are indexed by frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert redford's is an amazing actor and now the same being director norman's father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for retail and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also congratulations to the two little boy's that played the part's of norman and paul they were just brilliant children are often left out of the praising list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_sequence = \" \".join(inverted_word_index.get(i-3, '?') for i in X_train[0])\n",
    "decoded_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After reading the review, we can see that it is a positive one and hence 1 indicates a positive review. We can confirm the same with one of the negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 5, 198, 138, 254, 8, 967, 10, 10, 39, 4, 1158, 213, 7, 650, 7660, 1475, 213, 7, 650, 13, 215, 135, 13, 1583, 754, 2359, 133, 252, 50, 9, 49, 1104, 136, 32, 4, 1109, 304, 133, 1812, 21, 15, 191, 607, 4, 910, 552, 7, 229, 5, 226, 20, 198, 138, 10, 10, 241, 46, 7, 158] \n",
      "\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(X_train[500],\"\\n\")\n",
    "print(y_train[500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"? and that's why hard to rate br br from the adult point of view hmm student point of view i must say i fell nearly asleep here sure there is some laughing scene all the credit takes here eddie but that can't save the disney type of script and whole movie that's why br br 2 out of 10\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_sequence = \" \".join(inverted_word_index.get(i-3, '?') for i in X_train[500])\n",
    "decoded_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can clearly see that this is a negative review and hence the corresponding value of y_train is 0.\n",
    "\n",
    "##### Therefore 0 indicates a negative review and 1 indicates a positive review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We now check for the total number of unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words: \n",
      "88585\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of words: \")\n",
    "print(len(numpy.unique(numpy.hstack(X_train))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that there are almost 89000 words in total. We now check for average review length and also the standard deviation of the length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Length of Reviews:\n",
      "238.71364\n",
      "Standard Deviation of Reviews:\n",
      "176.49367364852034\n"
     ]
    }
   ],
   "source": [
    "review_length=[len(rev) for rev in X_train]\n",
    "print(\"Mean Length of Reviews:\")\n",
    "print(numpy.mean(review_length))\n",
    "print(\"Standard Deviation of Reviews:\")\n",
    "print(numpy.std(review_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "415.2073136485203\n"
     ]
    }
   ],
   "source": [
    "print(numpy.mean(review_length) + numpy.std(review_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The sum of the mean and the standard deviation in our case is 415, this tells us the average number of words in a majority of the reviews. To not miss out on much information in the reviews, we would consider 450 to be the maximum review length. Any review less than 450 would be padded with 0. Not padding would result in input reviews of variable lengths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing a Baseline Model - (Single 1D - CNN)\n",
    "Since the data has already been prepared and pre-processed we proceed to develop a baseline model.<br>\n",
    "- We would also be reloading the imdb dataset into the train and test sets with a limit of 10000 words, i.e. selecting the top occurring 10000 words in the reviews. This would eliminate the less frequent words that aren't strong enough to change the classification and also reduce the computational requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common = 10000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As mentioned earlier, we would now set a limit for the length of the reviews to 450 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad dataset to a maximum review length in words\n",
    "pad = 450\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=pad)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Baseline model will consist of:\n",
    "- An embedding layer which converts the integer representation of the words into word embeddings. We mention the maximum vocabulary size (10000 in our case) and also mention the input length (which in our case is 450).\n",
    "- One 1D Convolutional layer having 32 filters with a filter size of 3, we use \"same\" padding so that the input and the output has the same dimensions.\n",
    "- followed by a maxpooling layer\n",
    "- and then flatten the output of the pooling layer to give us a long vector\n",
    "- We then add a fully connected dense layer with 128 neurons\n",
    "- The last layer is the sigmoid layer which gives us the output between 0 & 1\n",
    "\n",
    "All the layers other than the last layer will be using the ReLU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 450, 32)           320000    \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 450, 32)           3104      \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 225, 32)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 7200)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               921728    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,244,961\n",
      "Trainable params: 1,244,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "model = Sequential()\n",
    "\n",
    "# Embedding\n",
    "model.add(Embedding(most_common, 32, input_length=pad))\n",
    "\n",
    "# First Convolution1D Layer\n",
    "model.add(Conv1D(32, kernel_size=3,\n",
    "                 padding='same', activation='relu'))\n",
    "\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# flatten and put a fully connected layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# sigmoid\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The above model summary tells us about the total number of parameters that are to be trained and also the output dimensions after each layer.\n",
    "- The none in the output shape represents the batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting and evaluating the baseline model\n",
    "- We now proceed to compile the model. The loss function to be optimised is binary_crossentropy.\n",
    "- The optimizer which we will be using is ADAM.\n",
    "- The metric which we would use to evaluate is Accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='Adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We first specify few variables such as batch size and epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size, number of classes, epochs\n",
    "batch_size = 128\n",
    "epochs = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We proceed to fit the model using the x_train and y_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "196/196 [==============================] - 9s 42ms/step - loss: 0.4665 - accuracy: 0.7459 - val_loss: 0.2726 - val_accuracy: 0.8880\n",
      "Epoch 2/12\n",
      "196/196 [==============================] - 8s 40ms/step - loss: 0.1957 - accuracy: 0.9241 - val_loss: 0.2699 - val_accuracy: 0.8877\n",
      "Epoch 3/12\n",
      "196/196 [==============================] - 8s 41ms/step - loss: 0.1186 - accuracy: 0.9586 - val_loss: 0.3182 - val_accuracy: 0.8788\n",
      "Epoch 4/12\n",
      "196/196 [==============================] - 8s 40ms/step - loss: 0.0632 - accuracy: 0.9809 - val_loss: 0.3932 - val_accuracy: 0.8739\n",
      "Epoch 5/12\n",
      "196/196 [==============================] - 8s 41ms/step - loss: 0.0259 - accuracy: 0.9944 - val_loss: 0.5081 - val_accuracy: 0.8691\n",
      "Epoch 6/12\n",
      "196/196 [==============================] - 8s 41ms/step - loss: 0.0093 - accuracy: 0.9987 - val_loss: 0.5866 - val_accuracy: 0.8695\n",
      "Epoch 7/12\n",
      "196/196 [==============================] - 8s 43ms/step - loss: 0.0030 - accuracy: 0.9998 - val_loss: 0.6634 - val_accuracy: 0.8700\n",
      "Epoch 8/12\n",
      "196/196 [==============================] - 8s 41ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.7234 - val_accuracy: 0.8704\n",
      "Epoch 9/12\n",
      "196/196 [==============================] - 8s 42ms/step - loss: 4.4311e-04 - accuracy: 1.0000 - val_loss: 0.7732 - val_accuracy: 0.8708\n",
      "Epoch 10/12\n",
      "196/196 [==============================] - 8s 41ms/step - loss: 2.7437e-04 - accuracy: 1.0000 - val_loss: 0.8192 - val_accuracy: 0.8707\n",
      "Epoch 11/12\n",
      "196/196 [==============================] - 8s 41ms/step - loss: 1.8532e-04 - accuracy: 1.0000 - val_loss: 0.8629 - val_accuracy: 0.8710\n",
      "Epoch 12/12\n",
      "196/196 [==============================] - 8s 41ms/step - loss: 1.2834e-04 - accuracy: 1.0000 - val_loss: 0.9022 - val_accuracy: 0.8705\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24829e48888>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that the baseline model returns good train and validation accuracies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We now evaluate the baseline model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 3s 3ms/step - loss: 0.9022 - accuracy: 0.8705\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9021984934806824, 0.8704800009727478]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The accuracy is approximately 87%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing an Improved Model - (Double 1D - CNN)\n",
    "We now go ahead and check if it is possible to build an improved model that predicts the sentiment of the reviews with even higher accuracy.\n",
    "<br><br>\n",
    "We introduce another 1D Convolution Layer having 64 filters in this model.\n",
    "<br><br>\n",
    "All the layers other than the last layer will be using the ReLU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 450, 32)           320000    \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 450, 32)           3104      \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 450, 64)           6208      \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 225, 64)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 14400)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               1843328   \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,172,769\n",
      "Trainable params: 2,172,769\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "model1 = Sequential()\n",
    "\n",
    "# Embedding\n",
    "model1.add(Embedding(most_common, 32, input_length=pad))\n",
    "\n",
    "# First Convolution1D Layer\n",
    "model1.add(Conv1D(32, kernel_size=3, padding='same', activation='relu'))\n",
    "\n",
    "# Second Convolution1D Layer\n",
    "model1.add(Conv1D(64, kernel_size=3, padding='same', activation='relu'))\n",
    "\n",
    "model1.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# flatten and put a fully connected layer\n",
    "model1.add(Flatten())\n",
    "model1.add(Dense(128, activation='relu'))\n",
    "\n",
    "#sigmoid layer\n",
    "model1.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The above model summary tells us about the total number of parameters that are to be trained and also the output dimensions after each layer.\n",
    "- The none in the output shape represents the batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting and evaluating the first improved model\n",
    "- We now proceed to compile the model. The loss function to be optimised is binary_crossentropy.\n",
    "- The optimizer which we will be using is ADAM.\n",
    "- The metric which we would use to evaluate is Accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(loss='binary_crossentropy',\n",
    "              optimizer='Adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We proceed to fit the first improved model using the x_train and y_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "196/196 [==============================] - 19s 94ms/step - loss: 0.4971 - accuracy: 0.7128 - val_loss: 0.3107 - val_accuracy: 0.8634\n",
      "Epoch 2/12\n",
      "196/196 [==============================] - 19s 98ms/step - loss: 0.2017 - accuracy: 0.9224 - val_loss: 0.3034 - val_accuracy: 0.8745\n",
      "Epoch 3/12\n",
      "196/196 [==============================] - 17s 88ms/step - loss: 0.1234 - accuracy: 0.9559 - val_loss: 0.3175 - val_accuracy: 0.8781\n",
      "Epoch 4/12\n",
      "196/196 [==============================] - 17s 88ms/step - loss: 0.0693 - accuracy: 0.9777 - val_loss: 0.4432 - val_accuracy: 0.8719\n",
      "Epoch 5/12\n",
      "196/196 [==============================] - 17s 88ms/step - loss: 0.0369 - accuracy: 0.9897 - val_loss: 0.5093 - val_accuracy: 0.8726\n",
      "Epoch 6/12\n",
      "196/196 [==============================] - 17s 89ms/step - loss: 0.0200 - accuracy: 0.9953 - val_loss: 0.6050 - val_accuracy: 0.8623\n",
      "Epoch 7/12\n",
      "196/196 [==============================] - 17s 88ms/step - loss: 0.0138 - accuracy: 0.9970 - val_loss: 0.6982 - val_accuracy: 0.8589\n",
      "Epoch 8/12\n",
      "196/196 [==============================] - 17s 89ms/step - loss: 0.0222 - accuracy: 0.9928 - val_loss: 0.7855 - val_accuracy: 0.8602\n",
      "Epoch 9/12\n",
      "196/196 [==============================] - 17s 88ms/step - loss: 0.0197 - accuracy: 0.9934 - val_loss: 0.7710 - val_accuracy: 0.8662\n",
      "Epoch 10/12\n",
      "196/196 [==============================] - 17s 88ms/step - loss: 0.0075 - accuracy: 0.9979 - val_loss: 0.8617 - val_accuracy: 0.8646\n",
      "Epoch 11/12\n",
      "196/196 [==============================] - 18s 91ms/step - loss: 0.0050 - accuracy: 0.9987 - val_loss: 0.9407 - val_accuracy: 0.8614\n",
      "Epoch 12/12\n",
      "196/196 [==============================] - 18s 90ms/step - loss: 0.0038 - accuracy: 0.9986 - val_loss: 0.9787 - val_accuracy: 0.8580\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x248301ee1c8>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " model1.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=12,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that the improved model has similar training and validation accuracy as our previous model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We now evaluate the first improved model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 5s 6ms/step - loss: 0.9787 - accuracy: 0.8580\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9787335395812988, 0.8580399751663208]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The accuracy very slightly reduces but is similar to that of our previous model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing a Second Improved Model - (LSTM 1D - CNN)\n",
    "We now go ahead and check if it is possible to build another improved model that predicts the sentiment of the review with even higher accuracy.\n",
    "<br><br>\n",
    "We remove the second 1D Convolution Layer and add a LSTM Layer with 128 neurons and a dropout of 0.2 in this model.\n",
    "<br><br>\n",
    "All the layers other than the last layer will be using the ReLU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 450, 32)           320000    \n",
      "                                                                 \n",
      " conv1d_5 (Conv1D)           (None, 450, 32)           3104      \n",
      "                                                                 \n",
      " max_pooling1d_3 (MaxPooling  (None, 225, 32)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 128)               82432     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 422,177\n",
      "Trainable params: 422,177\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "model2 = Sequential()\n",
    "\n",
    "# Embedding\n",
    "model2.add(Embedding(most_common, 32, input_length=pad))\n",
    "\n",
    "# First Convolution1D Layer\n",
    "model2.add(Conv1D(32, kernel_size=3, padding='same', activation='relu'))\n",
    "\n",
    "model2.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# LSTM Layer with Dropout\n",
    "model2.add(LSTM(128,dropout=0.2))\n",
    "model2.add(Dropout(0.25))\n",
    "\n",
    "# flatten and put a fully connected layer\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(128, activation='relu'))\n",
    "\n",
    "#sigmoid layer\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The above model summary tells us about the total number of parameters that are to be trained and also the output dimensions after each layer.\n",
    "- The none in the output shape represents the batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting and evaluating the second improved model\n",
    "- We now proceed to compile the model. The loss function to be optimised is binary_crossentropy.\n",
    "- The optimizer which we will be using is ADAM.\n",
    "- The metric which we would use to evaluate is Accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss='binary_crossentropy',\n",
    "              optimizer='Adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We proceed to fit the second improved model using the x_train and y_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "196/196 [==============================] - 149s 752ms/step - loss: 0.5021 - accuracy: 0.7305 - val_loss: 0.3450 - val_accuracy: 0.8466\n",
      "Epoch 2/12\n",
      "196/196 [==============================] - 134s 686ms/step - loss: 0.2397 - accuracy: 0.9072 - val_loss: 0.3119 - val_accuracy: 0.8752\n",
      "Epoch 3/12\n",
      "196/196 [==============================] - 136s 696ms/step - loss: 0.1679 - accuracy: 0.9388 - val_loss: 0.3063 - val_accuracy: 0.8767\n",
      "Epoch 4/12\n",
      "196/196 [==============================] - 136s 696ms/step - loss: 0.1252 - accuracy: 0.9571 - val_loss: 0.3953 - val_accuracy: 0.8542\n",
      "Epoch 5/12\n",
      "196/196 [==============================] - 136s 696ms/step - loss: 0.1035 - accuracy: 0.9651 - val_loss: 0.3961 - val_accuracy: 0.8698\n",
      "Epoch 6/12\n",
      "196/196 [==============================] - 136s 695ms/step - loss: 0.0787 - accuracy: 0.9743 - val_loss: 0.4364 - val_accuracy: 0.8624\n",
      "Epoch 7/12\n",
      "196/196 [==============================] - 136s 697ms/step - loss: 0.0593 - accuracy: 0.9804 - val_loss: 0.4689 - val_accuracy: 0.8632\n",
      "Epoch 8/12\n",
      "196/196 [==============================] - 136s 696ms/step - loss: 0.0409 - accuracy: 0.9869 - val_loss: 0.6248 - val_accuracy: 0.8613\n",
      "Epoch 9/12\n",
      "196/196 [==============================] - 137s 699ms/step - loss: 0.0353 - accuracy: 0.9888 - val_loss: 0.6102 - val_accuracy: 0.8597\n",
      "Epoch 10/12\n",
      "196/196 [==============================] - 136s 696ms/step - loss: 0.0307 - accuracy: 0.9905 - val_loss: 0.6490 - val_accuracy: 0.8621\n",
      "Epoch 11/12\n",
      "196/196 [==============================] - 137s 701ms/step - loss: 0.0322 - accuracy: 0.9894 - val_loss: 0.6509 - val_accuracy: 0.8630\n",
      "Epoch 12/12\n",
      "196/196 [==============================] - 138s 702ms/step - loss: 0.0334 - accuracy: 0.9888 - val_loss: 0.7024 - val_accuracy: 0.8621\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x248301f1108>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " model2.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=12,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that this model also has similar train and validation accuracy as our previous models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We now evaluate the second improved model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 40s 51ms/step - loss: 0.7024 - accuracy: 0.8621\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7024185061454773, 0.8621199727058411]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The accuracy is similar to that of our previous models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing a Third Improved Model - (GRU 1D - CNN)\n",
    "We now go ahead and check if it is possible to build another improved model that predicts the sentiment of the review with even higher accuracy.\n",
    "<br><br>\n",
    "We remove LSTM Layer and add a GRU Layer with 128 neurons and a dropout of 0.2 in this model.\n",
    "<br><br>\n",
    "All the layers other than the last layer will be using the ReLU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, 450, 32)           320000    \n",
      "                                                                 \n",
      " conv1d_6 (Conv1D)           (None, 450, 32)           3104      \n",
      "                                                                 \n",
      " max_pooling1d_4 (MaxPooling  (None, 225, 32)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 128)               61824     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 401,569\n",
      "Trainable params: 401,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "model3 = Sequential()\n",
    "\n",
    "# Embedding\n",
    "model3.add(Embedding(most_common, 32, input_length=pad))\n",
    "\n",
    "# First Convolution1D Layer\n",
    "model3.add(Conv1D(32, kernel_size=3, padding='same', activation='relu'))\n",
    "\n",
    "model3.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# GRU Layer with Dropout\n",
    "model3.add(GRU(128,dropout=0.2))\n",
    "model3.add(Dropout(0.25))\n",
    "\n",
    "# flatten and put a fully connected layer\n",
    "model3.add(Flatten())\n",
    "model3.add(Dense(128, activation='relu'))\n",
    "\n",
    "#sigmoid layer\n",
    "model3.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The above model summary tells us about the total number of parameters that are to be trained and also the output dimensions after each layer.\n",
    "- The none in the output shape represents the batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting and evaluating the third improved model\n",
    "- We now proceed to compile the model. The loss function to be optimised is binary_crossentropy.\n",
    "- The optimizer which we will be using is ADAM.\n",
    "- The metric which we would use to evaluate is Accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.compile(loss='binary_crossentropy',\n",
    "              optimizer='Adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We proceed to fit the third improved model using the x_train and y_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "196/196 [==============================] - 111s 557ms/step - loss: 0.4875 - accuracy: 0.7448 - val_loss: 0.3150 - val_accuracy: 0.8664\n",
      "Epoch 2/12\n",
      "196/196 [==============================] - 109s 555ms/step - loss: 0.2315 - accuracy: 0.9082 - val_loss: 0.2806 - val_accuracy: 0.8839\n",
      "Epoch 3/12\n",
      "196/196 [==============================] - 109s 557ms/step - loss: 0.1680 - accuracy: 0.9375 - val_loss: 0.3457 - val_accuracy: 0.8654\n",
      "Epoch 4/12\n",
      "196/196 [==============================] - 2442s 13s/step - loss: 0.1187 - accuracy: 0.9589 - val_loss: 0.3678 - val_accuracy: 0.8696\n",
      "Epoch 5/12\n",
      "196/196 [==============================] - 118s 603ms/step - loss: 0.0886 - accuracy: 0.9702 - val_loss: 0.4800 - val_accuracy: 0.8558\n",
      "Epoch 6/12\n",
      "196/196 [==============================] - 114s 581ms/step - loss: 0.0692 - accuracy: 0.9770 - val_loss: 0.5835 - val_accuracy: 0.8639\n",
      "Epoch 7/12\n",
      "196/196 [==============================] - 110s 563ms/step - loss: 0.0655 - accuracy: 0.9770 - val_loss: 0.5113 - val_accuracy: 0.8660\n",
      "Epoch 8/12\n",
      "196/196 [==============================] - 110s 563ms/step - loss: 0.0439 - accuracy: 0.9868 - val_loss: 0.5413 - val_accuracy: 0.8625\n",
      "Epoch 9/12\n",
      "196/196 [==============================] - 111s 569ms/step - loss: 0.0302 - accuracy: 0.9904 - val_loss: 0.7006 - val_accuracy: 0.8651\n",
      "Epoch 10/12\n",
      "196/196 [==============================] - 112s 574ms/step - loss: 0.0422 - accuracy: 0.9853 - val_loss: 0.6127 - val_accuracy: 0.8661\n",
      "Epoch 11/12\n",
      "196/196 [==============================] - 114s 582ms/step - loss: 0.0180 - accuracy: 0.9942 - val_loss: 0.6939 - val_accuracy: 0.8602\n",
      "Epoch 12/12\n",
      "196/196 [==============================] - 117s 596ms/step - loss: 0.0151 - accuracy: 0.9955 - val_loss: 0.7988 - val_accuracy: 0.8645\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2483a5289c8>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " model3.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=12,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that this model also has similar train and validation accuracy as our previous models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We now evaluate the third improved model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 44s 57ms/step - loss: 0.7988 - accuracy: 0.8645\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.798789918422699, 0.8645200133323669]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The accuracy is similar to that of our previous models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### It can be observed that all the models are similar in performance in terms of accuracy. The simple 1D CNN model, however, was the fastest to train. The accuracy for this model also seemed minutely more than that of the other models. We would hence consider this as the finalised model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions using the finalized model\n",
    "- We now test the model on the complete Imdb review data instead of dividing it into test and train sets to get an idea of how the model performs.\n",
    "- We will need to concatenate the test and train datasets since load_data gives 4 outputs. We also need to configure the maximum number of frequent words and the number of words in a review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=10000)\n",
    "# pad dataset to a maximum review length in words\n",
    "pad = 450\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=pad)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=pad)\n",
    "X = numpy.concatenate((X_train, X_test), axis=0)\n",
    "y = numpy.concatenate((y_train, y_test), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We check for the shape of the x and y data for our verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " test data\n",
      "(50000, 450)\n",
      "(50000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n test data\")\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We now evaluate the finalised 1D CNN model on the complete dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.4511 - accuracy: 0.9352\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4511488676071167, 0.9352399706840515]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The final accuracy is near 93%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions\n",
    "- We predict the sentiment for a few reviews manually using the model.\n",
    "- We now load a decoded review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? this is exactly the reason why many people remain homeless because stupid producers pay their money to make awful films like this instead of ? if they can bother br br this film is even worse than white chicks little man has a lame excuse for posing a character midget as a baby story is awful considering it was written by six people the idea still wouldn't be too bad though if it was original and not a rip off of a cartoon episode it has funny moments but some of them are way over done and some are just stupid the acting was very very bad so was the directing anyone involved in this film should be ashamed of themselves it is racist and very offensive to ? i mean instead of showing sympathy to them the film makers make fun of them it really ? me how they do it they see ? being just like babies and for a character who is a midget pretending to be an abandoned baby just to get a diamond from a certain family that is its lame excuse for showing something like that it just was not worth it don't watch this film it is a huge waste of time and money\""
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_sequence = \" \".join(inverted_word_index.get(i-3, '?') for i in X[1234])\n",
    "decoded_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The question marks represent the padding added to match the max length 450 words.\n",
    "- On reading, we can understand the above is a negative review. Let us now check what the model predicts for the same sample. We first predict for all the entire X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The predicted label of the sample review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.2858462e-19], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[1234]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that the predicted value is ~0 which tells us that the review is negative.\n",
    "- We now confirm it with the actual label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(y[1234])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We now check for the predictions of another sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? in the sea of crap that hollywood and others continue to put out this is one of those ? in the rough a small simple movie that is very entertaining and leaves you with the feeling that you didn't just waste an hour and a half of your life br br ashley judd is really quite amazing in this movie i had never really been a fan or had noticed her before but going back and seeing this early performance of hers convinced me she's extremely talented br br watching this film was an assignment in a college course for me so i was skeptical i would even care i thought oh boy some dumb ? flick or feminist male bashing indie crap i was pleasantly surprised without ? the many relevant themes i'll just say if you haven't seen it do yourself a favor and check it out sometimes the down to earth slice of life movies are the best and this is a great one\""
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_sequence = \" \".join(inverted_word_index.get(i-3, '?') for i in X[12345])\n",
    "decoded_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- On reading, we can understand the above is a positive review. Let us now check what the model predicts for the same sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9989635], dtype=float32)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[12345]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that the predicted value is ~1 which tells us that the review is positive.\n",
    "- We now confirm it with the actual label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(y[12345])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
